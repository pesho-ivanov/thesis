\section*{Analysis of scaling}
%\addcontentsline{toc}{section}{\protect\numberline{}{Analysis of scaling}}

% background
Traditionally, the runtime and memory have been analysed for the worst case
asymptotic behavior of the algorithm. Since the near quadratic worst case
asymptotics is likely to be tight for both global~\citep{backurs2015edit} and
semi-global alignment, we targeted related sequences (by limiting their
per-letter error rate), and estimated their empyric runtime and memory scaling.

% issue
The seed heuristic, which is in the core of our \A algorithm, is admissible
(optimistic) but not consistent (monotone). As a consequence, any of the
quadratic number of state can be expanded multiple times, which could
theoretically lead to over-quadratic scaling. In practice, repeated expansions
happen but the empyrical scaling is preserved near-linear as long as the number
of seed matches is near-linear and the seed heuristic is capable of compensating
for the errors.

% empyrical
Our empyrical analysis of scaling is done by repeatedly running the same
algorithm on increasingly more complex input data (\eg longer sequences, higher
error rate). This approach is useful to get a sense of the scaling of the
algorithm and its implementation but it is not asymptotical, includes noise (of
measurements and best-fit estimations), is not trivially applicable to more than
1 dimensions.

% theory
An alternative theoretical analysis would consider the average case (expected)
scaling of our algorithms under a data model. An imaginable result would look
like a connection between the sequence lengths, error rate, and the algorithm
steps (mostly dependant on the number of seed matches and the number of expanded
states). To construct such a connection, a heuristic function will have to be
chosen among a class of seed heuristics (\eg with certain seed length, allowed
number of errors in seed matches, etc.).

% regimes
Our algorithms seem to follow any of the scaling regimes, based on the
capability of the seed heuristic to compensate for all the errors.

% speculations
Based on the empitical evaluations and the intuition behind the algorithms, we
speculate that the sequence length until which our algorithms can scale
near-linearly, can be exponentially increased by lowering the error rate. For
the semi-global alignment, any unit of alignment cost that is not compensated by
the potential of the seed heuristic, leads to a deeper exploration of the trie,
which is grows exponentially until it saturate to quadratic.