\subsection{Setting} \label{GLOBALsec:evals-setup}

\paragraph{Synthetic data}
We measure the performance of aligners on a set of randomly-generated sequences.
Each set is parametrized by the number of sequence pairs, the sequence length
$n$, and the error rate $e$. The first sequence in a pair is generated by
concatenating $n$ i.i.d. letters from $\Sigma$. The second sequence is generated
by sequentially applying $\lfloor e{\cdot} n\rfloor$ edit operations (insertions,
deletions, and substitutions with equal probability) to the first sequence. Note
that errors can cancel each other, so the final distance between the sequences
is usually less than $\lfloor e{\cdot} n \rfloor$. In order to minimize the
measurement errors, each test consists of a number of sequence pairs.

\paragraph{Human data}
We consider two datasets%
\footnote{\url{https://github.com/RagnarGrootKoerkamp/astar-pairwise-aligner/releases/tag/datasets}}
of Oxford Nanopore Technologies (ONT) reads that are aligned to regions of the
telomere-to-telomere assembly of a human genome
CHM13~(v1.1)~\citep{nurk2022complete}. Statistics are shown in~\cref{GLOBALtab:hg}.

\begin{itemize}
  \item \emph{CHM13: Human reads without biological variation.} We randomly
        sampled $50$ reads of length at least $500\kbp$ from the first
        $\qty{12}{GB}$ of the ultra-Long ONT reads that were used to assemble
        CHM13. We removed soft clipped regions and paired each read to its
        corresponding reference region in CHM13\footnote{\url{https://github.com/RagnarGrootKoerkamp/bam2seq}}.
  \item \emph{NA12878: Human reads with biological variation.} We consider the
        long ONT MinION reads from a different reference sample
        NA12878~\citep{bowden2019sequencing} which was used to evaluate
        \wfa~\citep{marco2022optimal}. This dataset contains $48$ reads longer than
        $500\kbp$ that were aligned to CHM13.
\end{itemize}

\begin{table}[t]
	\ra{0.8}
  \centering
  \sffamily
  \setlength{\tabcolsep}{3pt}
  \begin{tabular}{
    l
    c
    S[table-format=3]
    S[table-format=3]
    S[table-format=3]
    l
    S[table-format=1.1]
    S[table-format=1.1]
    S[table-format=2.1]
    }
    & \multirow{2.7}{4.8em}{\centering\textbf{Biological\newline
                       variation}} & \multicolumn{3}{c}{\textbf{Length} [\kbp]}
    &  & \multicolumn{3}{c}{\textbf{Error rate} [$\%$]} \\
    \cmidrule{3-5} \cmidrule{7-9}
    \textbf{Dataset} &&  {min} & {mean} & {max}  && {min} & {mean} & {max} \\
    \cmidrule{1-9}
    \datasetOne & No & 500 & 590 & 840 & & 2.7 & 6.3 & 18.0 \\
    \datasetTwo & Yes & 502 & 624 & 1053 & & 4.4 & 7.4 & 19.8 \\
    \cmidrule{1-9}
  \end{tabular}
  \caption[Statistics on the real data]{Statistics on the \textbf{real data}:
  ONT reads from human samples.}
  \label{GLOBALtab:hg}
\end{table}

\paragraph{Compared algorithms and aligners}
We compare the \sh and \csh, implemented in \astarpa, to the state-of-the-art exact
pairwise aligners \wfa and \edlib. In order to study the performance of the \A
heuristic functions and the pruning optimization, we also compare to \dijkstra's
algorithm (which is equivalent to \A with a zero heuristic) and to a no-pruning
variant of \A, all implemented in \astarpa. The exact aligners \seqan and
\parasail are not included in this evaluation since they have been outperformed
by \wfa and \edlib~\citep{marco2021fast}. We execute all aligners with unit edit
costs and compute not only the edit distance but also an optimal alignment.
See~\cref{GLOBALsec:app-tools} for aligner versions and parameters.

\paragraph{Execution}
All evaluations are executed on Arch Linux on a single thread on an
\texttt{Intel Core i7-10750H \mbox{CPU @ 2.6GHz}} processor with $\qty{64}{GB}$
of memory and $6$ cores, with hyper-threading and performance mode disabled. We fix
the CPU frequency to \texttt{2.6GHz} and limit the available memory per
execution to $\qty{30}{GB}$ using \texttt{ulimit -v 30000000}. To speed up the
evaluations, we run $3$ jobs in parallel, pinned to cores $0$, $2$, and $4$.

\paragraph{Measurements}
The runtime (wall-clock time) and memory usage (resident set size) of each run
are measured using \texttt{time}. The runtime in all plots and tables refers to
the average alignment time per pair of sequences. To reduce startup overhead,
we average faster alignments over more pairs of sequences, as specified in the figure
captions. Memory usage is measured as the maximum used memory during the
processing of the whole input file. To estimate how algorithms scale with
sequence length, we calculate best fit polynomials via a linear fit in the
log-log domain using the least squares method.

\paragraph{Parameters for the \A heuristics}
Longer sequences contain more potential locations for off-path seed matches
(\ie lying outside of the resulting optimal alignment). Each match takes time
to be located and stored, while also potentially worsening the heuristic. To
keep the number of matches low, longer seeds are to be preferred. On the other
hand, to handle higher error rates, a higher number of shorter reads is
preferable. For simplicity, we fix $k{=}15$ throughout our evaluations as a
reasonable trade-off between scaling to large $n$ and scaling to high $e$. We
use exact matches ($r{=}1$) for low error rates ($e {\leq} 5\%$), and inexact
matches ($r{=}2$) for high error rates ($e{\geq}10\%$). For human
datasets we use $r{=}2$ since they include sequences with error rates
higher than $10\%$.
