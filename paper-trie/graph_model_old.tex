\section{Graph model (old)}
%Let us be given a PNFA model with a single starting node called a \textit{supersource}.
We will use notations similar to \cite{pfau2010probabilistic}. Let a PNFA be 5-tuple $M = (Q, \Sigma, \delta, \pi, q_0)$ where $Q$ is a finite set of states, $\Sigma$ is a finite alphabet of symbols, $\delta \subseteq Q \times \Sigma \times Q$ is the transition function (note that as the automata is non-deterministic, the same state/symbol pair may repeat). For brevity, we will use $l(e) := label$ where $e \in Q \times \{ label \} \times Q$, $to(e) := v$ where $e \in Q \times \Sigma \times \{v\}$. $\pi : e \to [0, 1]$ is the probability of transiting through the edge $e \in \delta$. $q_0$ is the initial superstate.

\subsection{Sequence generation by a graph reference}
Given the PNFA $G$ and a path length $L$, we will generate a sequence with length $L$ doing a random walk in $G$. If for every node, there is an outgoing non-$\epsilon$ edge with positive probability, the algorithm terminates with probability $1$. In order to generate a read, the generated sequence can be modified using additional phred values.

\subsection{Inference on graphs}
%Find the optimum alignment starting at a given position in the graph.
%The only difference from the (global) mapping is initializing the algorithms with only a single starting node instead of all.
%\paragraph{Global alignment}
%Add a supersource connected to all the ``towers''.

Definitions:

\begin{align*}
	\hat{r} &\text{ --- query read with base-calling error probabilities $P(bp_i \in \hat{r})$}
    G &\text{ --- PNFA (with $\epsilon$ edges)} \\
    spell(path) &= \text{$path$ with removed $\epsilon$-letters} \\
	score_G(e) &= - 10 \log_{10} P_G(e)  \text{ --- phred-like score for an edge} \\
	phred(r_i) &= - 10 \log_{10} P(r_i)  \text{ --- phred value for a base pair in the read $r$} \\
	P(r_i) &= 10^{- phred(r_i) / 10}  \text{ --- base-calling error probability} \\
\end{align*}

Optimization task in terms of probabilities:

\begin{align*}
    P(path \mid G) &= \smashoperator{\prod_{e \in edges(path)}} P_G(e) \\
	P(spell \mid \hat{r}) &= \smashoperator{\prod_{spell_i = \hat{r_i}}} (1 - P(\hat{r_i})) \smashoperator{\prod_{\substack{spell_i != \hat{r_i}}}} \frac{1}{3} P(\hat{r_i}) \\
	\text{\fxwarning{MAQ uses}} P(spell \mid \hat{r}) &= \smashoperator{\prod_{\substack{spell_i != \hat{r_i}}}} P(\hat{r_i}) \\
    P(path \mid \hat{r}) &= P(path \mid G) P(spell(path) \mid \hat{r}) \\
	map_G(\hat{r}) &= \smashoperator{\argmax_{path: \lvert spell(path) \rvert = \lvert \hat{r} \rvert}} P(path | \hat{r}) \\
		&= \bf{ \smashoperator{\prod_{e \in edges(path)}} P_G(e) \, \smashoperator{\prod_{spell_i = \hat{r_i}}} (1 - P(\hat{r_i})) \, \smashoperator{\prod_{\substack{spell_i != \hat{r_i}}}} \frac{1}{3} P(\hat{r_i}) } \\
\end{align*}

Any mapping probability $P(path \mid \hat{r})$ can be deconstructed into three components corresponding to read sequencing errors, variant edges and edit edges.
We transforme all probabilities to phred-like values to formulate an equivalent task (details in the Supplementary). 

\begin{align*}
	score(path \mid \hat{r})
		&= \bf{\smashoperator{\sum_{e \in edges(path)}} score_G(e)
			+ \smashoperator{\sum_{spell_i=\hat{r_i}}} - 10 \log_{10} (1-P(\hat{r_i}))
			+ \smashoperator{\sum_{\substack{spell_i!=\hat{r_i}}}} \frac{1}{3} phred(\hat{r_i}) } \\
	map_G(\hat{r}) &\equiv \smashoperator{\argmin_{path: \lvert spell(path) \rvert = \lvert \hat{r} \rvert}} score(path | \hat{r}) \\
\end{align*}

\fxnote{compare with the ``correct alignment probability'' by \todo{add citation: li2008mapping}}
\fxnote{prior on starting vertices}
``probability p(z|x,u) of z coming from the position u equals the product of the error probabilities of the mismatched bases at the aligned position.'' \todo{add citation: li2008mapping}


\subsection{Length-normalized alignment probability} \fxwarning{Is there a parallel with the generative model?}
In order to compensate for the monotonically decreasing probability with read length, we propose a length-normalized probability.
It can be used as an intuitive alignment quality score, namely ``the average alignment probability per read letter'',
	as a threshold parameter (substituting for the usual edit distance) together with a minimal alignment length parameter (above),
	or as an optimization criteria (algorithm presented next).

If a read with length $L$ aligns with an (additive) score $s$, the normalized alignment score is $\bar{s} = s / L$.
Thus (using definition), the normalized (multiplicative) probability becomes $\bar{p} = p^{1/L}$ where $p = 10^{-10 s}$ (obviously, $\bar{p} \in [0, 1]$ as well).
Theorem: After the Dijkstra algorithm finds an optimal alignment, the optimal alignments for all prefixes of the read have been inserted to the queue. Proof: From the contrary. \fxnote{Formal proof.}
Thus, during the Dijkstra algorithm, we can keep the optimal subsolutions (for each read prefix) and then normalize each of them by the corresponding length.

In order to prioritize longer alignments, the alignment score should also include a length-dependent member \fxwarning{Can this be explained by the generative model?}.
Naturally, a probability factor of $0.25^{L-\textit{aligned}}$ that punishes short alignments follows a generative process that uniformly at random assignes a nucleotide to each read position that has been read but happens to be after the known reference region.
This factor can be extended to depend on the read letters that remained unaligned: for example, it can fit more closely to a known distribution of nucleotides in any genome. 


%WARNING: Separate optimization for left and right does not guarantee neither existance of a prefix+suffix meeting in the same node nor global optimality if such node exists for some pair of a suffix and a prefix!!!
\subsection{Partial read alignment}
The work \cite{jain2019complexity} uses a sequence of dummy vertics in order to accomodate for a query prefix that is not included in the graph.

It may be necessary to align only part of the read to the reference
	(\eg because of adapters on the read or because of the read covering genome parts that are not covered by the reference).
We call an alignment \textit{partial} if it is allowed to skip the alignmening of variable-length prefix and/or suffix of the read.
We call \textit{best partial alignment} a partial alignment that maximizes the length-normalized probability of the alignment.
The best partial alignment can be seen as a generalization of MEMs that is widely used (Table \ref{table:comparison}).
We call \textit{a best partial alignment through a pivot} we call a best partial alignment that includes a certain read position called \textit{pivot}.
%The algorithm solving this task exactly does not increase the overall alignment complexity. \fxwarning{Not true if many trials are needed.}

To find a best partial alignment through a pivot position, we align the read part to the left ($r_l$) and to the right ($r_r$) of it.
The right part we can align using the same procedure for mapping the whole read
	and the left part can be aligned starting from the pivot and going backwards through the reverse edges.

Theorem: After optimally solving the global task using Dijkstra, all local solutions (for all prefixes) will also be found as subsolutions.
Proof: Obvious. \fxnote{Formal proof.}

Theorem: Solving the reverse tasks for $r_l$ (iterating the read to the left of the pivot and using the reverse graph edges)
	finds the same shortest paths as solving the forward tasks for all suffixes of $r_l$.
Proof: Obvious. \fxnote{Formal proof.}

Theorem: If the optimal solutions of the two parts align the pivot of the read to the same vertex, then the concatenation of the subtasks solve the whole task optimally.
%Proof: Among the optimal subsolutions, there will always exist a suffix of $r_l$ and a prefix of $r_r$ such that meet in the same vertex on which the pivot aligns to the optimal $r$ alignment (assuming there is only one best alignment; otherwise lists of best subsulutions should be considered and matched). \fxnote{Formal proof.}

A possible strategy for choosing the pivot could be the middle of the read, or trying several pivots at different parts of the read.
Obviously, a minimal aligned read length can be enforced by choosing the left and the right alignement ends some distance apart.
%This can be solved for $O(L)$ using the sliding window technique (note that the subsolution probabilities/scores are not anymore monotonic on the number of aligned read letters).
%
%For the partial alignment, the score can be minimized (respectively, the probability maximized) by picking the left and the right independent optimums.

\subsection{Gap between the biological generative model and the graph model}
The biological generative model represents the distribution $p_b(y=(genome, position), x=read)$ which is the most detailed model we consider. The graph generative models represent the distribution $p_g(y=node, x=read)$ which is included in and deterministically computable by $p_b$ ($node$ is the starting node where a read is mapped). So there exists a surjective function $f: (genome, position) \to node$ that represents the transition from genome coordinates to graph nodes. The gap to be optimized is between the distribution $p_g$ and the the original distribution transferred to the starting nodes domain:

\begin{align*}
	p^*_g(node, read) &= \sum_{\substack{genome \land position: \\f(genome, position)=node}} p_b((genome, position), read) \\
	%& \argmax_\theta P(\theta \mid S) = \argmax_\theta \prod_{s \in S} map_\theta(s) \\
	gap(p^*_g, p_g \mid \theta) &= \kld{p^*_g}{p_g(\theta)} \\
	\text{where}& \\
	S &\text{ --- the set of observed sequences} \\
	p_G &\text{ --- the sequence distribution of the generative graph model $G$} \\
	\theta &\text{ --- the edge probabilities (incl. edit probabilities)}
\end{align*}

%Consider a trivial transition graph (without edit operations) $G_T$ with $N$ paths starting from one supersource vertex, branching right after the start (each branch with probability $1/N$), going straight all the time (with probability $1$) and merging to a supersink vertex. Such a graph represents the \textit{ideal case} in the sense that the graph preserves all global information about the genomes without collapsing nodes (e.g. with equal kmers in a DBG). We provide an algorithm for generating all collapsed graphs by transforming the non-collapsed graph by sequences of \textit{collapse} operations.

%\begin{hyp}[H\ref{hyp:first}] \label{hyp:first}
%	The correct alignment probability on a non-collapsed graph is not lower than on any collapsed graph built on the same genome set.
%\end{hyp}



\section{Graph model construction}

The PNFA captures the probabilistic nature of large-scale variation (by the reference genomes used for construction) as well as of small-scale variation (by the edit edges). It can be used as both a generative model and a reference for mapping new reads. The PNFA accommodates for edit operations and can be uniquely constructed from a pDFA given the edit probabilities \ref{fig:models}. Each edge in the pDFA corresponds to exactly one letter. The pDFA can be uniquely constructed from a pDBG or another string graph with edge probabilities. Interestingly, the graph structure is computed independently of the edge probabilities so all existing string graph approaches can be reused and extended. The probability of an outgoing edge is in $[0; 1]$ and all outgoing edges from a vertex may or may not form distribution depending on the desired properties.

\subsection{Estimating edge probabilities}
%Given the graph structure of any generative model, we formulate two optimization problems solved by varying the transition probabilities on the edges (possibly shrinking the possible parameters by fixing the edit operations to have equal probabilities). The first problem is maximizing the probability of generating observed reads or genomes by solving the inference problem on the observed sequences (e.g. reads or genomes) --- this is tricky as $map$ includes getting $argmax$. The second optimization problem is minimizing the difference between statistics of observed sequences and of reads produced by the generative model. Depending on the goal, any of them or a combination can be used.

\fxnote{Compare with the probabilities estimation of Partis'16\cite{ralph2016consistency}.}
\fxnote{Compare with FORGe'18\cite{pritt2018forge}(preprint).}

The learning process optimizes the edge probabilities in order to minimize the gap between the distribution graph generative model and the transformed distribution of the biological generative model.

\begin{align*}
	& \argmin_\theta gap(p^*_g, p_g \mid \theta) \\
\end{align*}

\paragraph{From a set of reference sequences}
In case the graph structure is defined by the same set of sequences which is to be used for edge probabilities optimization, then no edits are expected. In this case it is reasonable to perform the optimization on a graph model without edit edges (i.e. pDBG or pDFA). Errors in the reads are expected to be taken care of by assigning low probabilities.

The optimization task can then be solved exactly by counting.\fxwarning{To prove}

\paragraph{From independent graph and a set of sequences}
In the most general case when the graph structure is not assumed to be build by the given set of sequences, the optimization can be performed on the PNFA model which accounts for both edits and technical errors (in case the observed sequences include phred values).

%\paragraph{From a set of polymorphisms}
%Given a non-probabilistic DBG (or a linear genome) and a list of single-nucleotide polymporphism frequences 

Construction from a set of reads: every kmer from the hidden reference is seen in the reads. (+ the noise is improbable to give a better alignment)
\fxnote{how to compute the transition probabilities (construction) and what is their benefit; how do the HMM paper solves the problem of comparing short-vs-long paths and low complexity regions vs high complexity regions \cite{ralph2016consistency}}

\paragraph{From a reference sequence and a set of known variants}

\paragraph{From a sequence graph or PRG}

\subsection{pDBG to pDFA}
We define a probabilistic DBG (pDBG) as a generalization of a DBG associating an additional transition probability to each edges. If not stated explicitly, we assume that the outgoing probabilities from each non-terminal vertex form a distribution (sum up to 1). Naturally defined by the DBG, every transition is associated with a the rightest letter of the kmer of the receiving vertex. In order to support paths with edits, we generalize the letters by using not only nucleotides but also gap letters ($\epsilon$). We can the look at the pDBG also as a PNFA which is a generalization of the discrete-time discrete-state Markov process with transition labels. Note that the PNFA is a generalization of NFA so there may be repeated outgoing letters and epsilon transition.

For every node in the pDBG, $k$ nodes (a tower up) are added to the PNFA: for each non-empty prefix. A node for prefix length $i$ is connected to the node for prefix length $i+1$ with the $(i+1)$st letter. The new nodes allow for edits in the first kmer of a match. Added is a supersource having epsilon transitions to all nodes corresponding to prefixes of size 1 (transitions to longer prefixes would be equivalent to transitions to towers of another kmer nodes).

Construction is trivial when probabilities per edge are given along with the graph. In case no probabilities are supplied, they are assumed to be all 1 (with equivalent scores $-log(1)=0$). This may be tricky when mapping using DP as it induces cycles (because of deletion edit operations) that cannot be ordered correctly because of the 0 score penalties.

\paragraph{DBG->PNFA: Matching the first k letters as well}
Before aligning the query to a path, we have to align the prefix of the query to the kmers of each vertex. It is not trivial to align the query to a kmer as the best aligning prefix does not have to be the one leading to optimal alignment on the graph. Thus we are converting the dDBG to a PNFA by adding a new vertex for each of the k prefixes of each DBG vertex resulting in total of $kN$ new vertices. Each group of k vertices are sequentially connected and finally lead to the initial kmer vertex. In order to save space, the repetitive prefixes of equal size can be further unified to form a DAG. The size of the new graph becomes $O(E+Vk)$.

\subsection{Edit scores to probabilities}
Additional edges can be added to the pDBG to account for edit operations without worsening the algorithms complexity. Note that under the assumption of a correct alignment (\fxwarning{what does this mean? The situation here is strange, as both positions tell us similar pieces of information (about the prefix}). It seems more reasonable to only allow insertions/deletions/substitutions when walking around in the graph, and ignore the query for this. The situation here is strange, as both positions tell us similar pieces of information (about the prefix). It seems more reasonable to only allow insertions/deletions/substitutions when walking around in the graph, and ignore the query for this.), it is enough to condition the edit penalties (probabilities) only on the graph position and not on the query position.

Let $p_i$, $p_d$ and $p_s$ stand for insertion, deletion and substitution probabilities (for notational simplicity, we assume these probabilities are global and not dependent on the vertex; this can be trivially generalized). An edit-pDBG is defined by a pDBG and edit probabilities by adding 4(V+E) more edges and no additional vertices. The additional edges per edit operations we define as:
\begin{itemize}
	\item insertion (append to query): +4V edges: 4 edges are added for each vertex v: (v, v, letter, $p_i/4$) for letter in nucleotides
	\item deletion (erase from query): +E edges: one edge per existing edge $(u, v, letter, p) \to (u, v, \epsilon, p_d)$
	\item substitution: +3E parallel edges: $(u, v, oldletter, p) \to (u, v, newletter, ?/(3*outNum))$ for newletter != oldletter, \fxerror{should be $p*p_s/3$: we proceed with oldletter, but it was substituted}
	\item \fxnote{transposition?}
\end{itemize}

After adding all edit transitions, we normalize the outgoing probabilities for each vertex to 1. More complex operations like inversions, duplications, or highly variable copy number remain challenging.

\subsection{Quantifying the mapping quality}

Blast tutorial: https://www.ncbi.nlm.nih.gov/BLAST/tutorial/Altschul-1.html

The alignment accuracy can be measured in global (we refer to it as mapping) and local sense (we refer to it as alignment): is the region where a read is mapped correct; and how accurate is the local alignment along the reference\cite{frith2010parameters}. \tool has an accent on improving local alignment while preserving the global mapping accuracy.

``One of the computational (and modeling) challenges facing the field of pan-genomics is how to deal with data uncertainty propagation through the individual steps of analysis pipelines. To do so, the individual processing steps need to be able to take uncertain data as input and to provide a ‘level of confidence’ for the output made. This can, for instance, be done in the form of posterior probabilities. Examples where this is already common practice include read mapping qualities [155] and genotype likelihoods [156].Computing a reasonable confidence level commonly relies on weighing alternative explanations for the observed data. In the case of read mapping, for example, having an extensive list of alternative mapping locations aids in estimating the probability of the alignment being correct. A pan-genome expands the space of possible explanations and can, therefore, facilitate the construction of fairer and more informative confidence levels''\cite{computational2016pengenomics}.

Different alignment quality scores include $\gamma$-centroid (probabilistic) alignment, E-values\cite{frith2010parameters} (the expected number of mapping occuring by chance: dependent only on the size of the reference and the length of the alignment). \fxnote{Computing the marginalized probabilities for each position of an alignment may be needed.}

\fxwarning{Handle multiple good mapping positions. MAQ outputs a score of 0.}

The generative process can generate the same sequence by going through different paths.
The mapping solution finds only the path with maximum.
Any mapping in our approach is characterized by a total mapping probabiliy that represents the product of
	three probabilities: acounting for variant edges, edit edges and phred scores.
As such a probabilitiy is heavily dependent on read length, overall read quality, reference size, etc.,
	we suggest using another, comparatative, score as a signal for the certainty of mapping
	(i.e. the confidence of the mapping position while fixing the reference and the read).
As a comparative score we can use the ratio between the probability of the best mapping and the probability of the second best mapping.
We extend the MAQ\cite{maq2008mapping} technique by applying it to graphs, allowing for edits, and apploximating more precisely.
In our framework, the mapping probabilities are trivilly extracted when aligning.
To make the mapping quality approximation more accurate, we find the $K$ best mappings use sampling to evaluate the precision of our approximation.
We call a mapping \textit{unique} if the probability of alignment is above a certain threshold
	(in MAQ a mapping is called \textit{unique} if the second best mapping has more mismatches than the best one).

Statistical issues with the huge alignment space.

\begin{align*}
	& \sum_{path(v_1 \xrightarrow{e_1} v_2 \xrightarrow{e_2} \dots \xrightarrow{v_{k-1}} v_k) \colon v_k=u} P(path) 
	%\ge \min_\theta gap(p^*_g, p_g \mid \theta) \\
\end{align*}

\begin{algorithm}[H]
	\caption{Sum of path probabilities given a read ($\pm \epsilon$ guarantees)}\label{sumofpaths}
	\begin{algorithmic}[1]
%		\Function{PhredProb}{$\mathit{is\_correct}, \mathit{phred})$}
%			\If{$\mathit{is\_correct}$}
%				\Return $\mathit{phred2prob}(\mathit{phred})$
%			\Else
%				\Return $(1 - \mathit{phred2prob}(\mathit{phred})) / 3$
%			\EndIf
%		\EndFunction
%
%		\Statex
%
%		\Function{JointProb}{$e(u, v, \mathit{label}, \mathit{prob}): \mathit{Edge}, r_i(\mathit{letter}, \mathit{phred})$}
%			\If{$\mathit{label} = \epsilon$}
%				\State $\mathit{phred\_prob} \gets 1.0$
%			\Else
%				\State $\mathit{phred\_prob} \gets \mathit{PhredProb}(\mathit{label} = \mathit{letter})$
%			\EndIf
%			\Return $\mathit{prob} \ast \mathit{phred\_prob}$
%		\EndFunction
%
%		\Statex
%
%		% todo: write in score terms
%		\Function{SumOfPathProbsIntervalWithPhred}{$G: \mathit{Graph}, r: \mathit{Read}, \epsilon > 0: \mathit{Probability}$}
%			%\Statex \Comment{$P_{\mathit{pref}}(i,v)$ -- upper bound on the sum of all paths $source \leadsto v$ spelling $r[1:i]$}
%			\Statex \Comment{$P_{\mathit{suff}}(i,v)$ -- lower bound on the sum of the probs of all paths starting at $v$ and spelling $r[i:\mathit{end}]$. One such estimate is the probability of the best path or the $k$ best paths from $(i,v)$}
%			\State $s \gets \mathit{State}(i=0, v=\textit{supersource}(G), \mathit{P_{pref}}=1.0)$
%			\Statex \Comment{Sorting criteria determines number of steps} 
%			\State $Q \gets \mathit{PriorityQueue}(\{ s \}, sortby=\mathit{P_{pref}})$
%			\While{$\mathit{sum}( \mathit{P_{pref}}(q \in Q) ) - \mathit{sum}( \mathit{P_{suff}}(q \in Q) ) > \epsilon$}  \Comment{$\lvert Q \rvert > 0$}
%				\State $s \equiv \mathit{State}(i, u, p) \gets Q.pop()$
%				\ForAll{$v, \mathit{label}, \mathit{prev\_prob} \equiv e \in \mathit{G.outgoing\_edges}(u)$}
%					\State $\mathit{step\_prob} \gets \mathit{JointProb}(e, r[i])$
%					\State $\mathit{Q.add}(\mathit{State}(i, v, \mathit{prev\_prob} \ast \mathit{step\_prob}))$
%				\EndFor
%			\EndWhile
%			\Return $(lo=\mathit{sum}( \mathit{P_{suff}}(q \in Q), up=\mathit{sum}( \mathit{P_{pref}}(q \in Q) ) )$
%		\EndFunction
%
%		\Statex

		\Function{SumOfPathProbsIntervalNoPhred}{$G: \mathit{Graph}, r: \mathit{Read}, \epsilon > 0: \mathit{Probability}$}
			%\Statex \Comment{$P_{\mathit{pref}}(i,v)$ -- upper bound on the sum of all paths $source \leadsto v$ spelling $r[1:i]$}
			%\Statex \Comment{$P_{\mathit{suff}}(i,v)$ -- lower bound on the sum of the probs of all paths starting at $v$ and spelling $r[i:\mathit{end}]$. One such estimate is the probability of the best path or the $k$ best paths from $(i,v)$}
			\State $s \gets \mathit{State}(i=0, v=\textit{supersource}(G), \mathit{P_{pref}}=1.0)$
			\Statex \Comment{Sorting criteria determines number of steps} 
			\State $Q \gets \mathit{PriorityQueue}(\{ s \}, sortby=\mathit{P_{pref}})$
			\While{$\mathit{sum}_{q \in Q}( \mathit{P_{pref}}(q) P_{best}(q) ) - \mathit{sum}( \mathit{P_{suff}}(q \in Q) ) > \epsilon$}  \Comment{$\lvert Q \rvert > 0$}
				\State $s \equiv \mathit{State}(i, u, p) \gets Q.pop()$
				\ForAll{$v, \mathit{label}, \mathit{prev\_prob} \equiv e \in \mathit{G.outgoing\_edges}(u)$}
					\If{$label = r[i] \textbf{ or } label = \epsilon$}
						%\State $\mathit{step\_prob} \gets \mathit{JointProb}(e, r[i])$
						\State $\mathit{Q.add}(\mathit{State}(i, v, \mathit{prev\_prob} \ast \mathit{prob}(e)))$
					\EndIf
				\EndFor
			\EndWhile
			\Return $(lo=\mathit{sum}( \mathit{P_{suff}}(q \in Q), up=\mathit{sum}( \mathit{P_{pref}}(q \in Q) ) )$
		\EndFunction

		\Statex

		\Function{PathCertainty}{$best\_path \colon Path, G \colon \mathit{Graph}, r \colon \mathit{Read}, \delta = 0.05 \colon \mathit{Accuracy}$}
			\State $\textit{best\_prob} \gets \textit{PathProb}(\textit{best\_path}, r)$
			\State $\epsilon \gets \delta \ast \textit{best\_prob}$
			\State $\textit{sum\_prob} \gets \textit{SumOfPathProbsInterval}(G, r, \epsilon)$
			\Return $( \textit{lo} = \textit{best\_prob} / \textit{up}(\textit{sum\_prob}), \textit{up} = \textit{best\_prob} / \textit{lo}(\textit{sum\_prob}) )$
		\EndFunction
	\end{algorithmic}
\end{algorithm}

%We define the full uncertainty of a mapping as:
%\begin{align*}
%	map_{\theta}(\hat{r}) &= \argmax_{\hat{g} \in U \wedge pos} P_{occ}(g) P_{mut}^{g}(\hat{g}) P_{start}^{g}(pos) P_{ph}(\hat{r}, r) \\
%			\text{where}& \\
%			& \theta = \langle P_{occ}, P_{mut}, P_{start}, P_{ph} \rangle \\
%\end{align*}
