\subsection*{Results}

We empirically demonstrate the superior scaling and performance to other optimal
alignment algorithms.

\paragraph{Scaling with reference size using a trie index}
\paragraph{Scaling with query length using a seed heuristic}
\paragraph{Scaling with error rate using inexact matching and chaining}

% background
Traditionally, the runtime and memory have been analysed for the worst case
asymptotic behavior of the algorithm. Since the near quadratic worst case
asymptotics is likely to be tight for both global~\citep{backurs2015edit} and
semi-global alignment, we targeted related sequences (by limiting their
per-letter error rate), and estimated their empyric runtime and memory scaling.

% issue
The seed heuristic, which is in the core of our \A algorithm, is admissible
(optimistic) but not consistent (monotone). As a consequence, any of the
quadratic number of state can be expanded multiple times, which could
theoretically lead to over-quadratic scaling. In practice, repeated expansions
happen but the empyrical scaling is preserved near-linear as long as the number
of seed matches is near-linear and the seed heuristic is capable of compensating
for the errors.

% empyrical
Our empyrical analysis of scaling is done by repeatedly running the same
algorithm on increasingly more complex input data (\AG longer sequences, higher
error rate). This approach is useful to get a sense of the scaling of the
algorithm and its implementation but it is not asymptotical, includes noise (of
measurements and best-fit estimations), is not trivially applicable to more than
1 dimensions.

% theory
An alternative theoretical analysis would consider the average case (expected)
scaling of our algorithms under a data model. An imaginable result would look
like a connection between the sequence lengths, error rate, and the algorithm
steps (mostly dependant on the number of seed matches and the number of expanded
states). To construct such a connection, a heuristic function will have to be
chosen among a class of seed heuristics (\AG with certain seed length, allowed
number of errors in seed matches, etc.).

% regimes
Our algorithms seem to follow any of the scaling regimes, based on the
capability of the seed heuristic to compensate for all the errors.

% speculations
Based on the empitical evaluations and the intuition behind the algorithms, we
speculate that the sequence length until which our algorithms can scale
near-linearly, can be exponentially increased by lowering the error rate. For
the semi-global alignment, any unit of alignment cost that is not compensated by
the potential of the seed heuristic, leads to a deeper exploration of the trie,
which is grows exponentially until it saturate to quadratic.

\paragraph{Scaling with reference size}
In~\cref{ch:trie} we present the tool \astarix which applies the \A algorithm to
find optimal alignments, based on a domain-specific heuristic and enhanced by
multiple algorithmic optimizations. Importantly, our approach allows for both
cyclic and acyclic graphs including variation and de Bruijn graphs. We
demonstrate that using a trie index we can achieve sublinear scaling of aligning
runtime with reference size, and that \A can scale exponentially better than
\dijkstra with increasing (but small) number of errors in the reads. Moreover,
for short reads, both \astarix and \dijkstra scale better and outperform current
state-of-the-art optimal aligners with increasing genome graph size.
Nevertheless, scaling optimal alignment of long reads on big graphs remained an
open problem.

\paragraph{Scaling with query length}
In~\cref{ch:seed} we upgrade \astarix with a novel \sh which guides the \A
search by preferring crumbs on nodes that lead towards optimal alignments even
for long reads. This approach enables the scaling of semi-global alignment with
read length.

\paragraph{Scaling with error rate}
In~\cref{ch:global} we resolve the third major bottleneck -- handling high error
rates. We presented an algorithm with an implementation in \astarpa solving
pairwise alignment between two sequences. The algorithm is based on \A with a
\sh, inexact matching, match chaining, and match pruning, which we proved to
find an exact solution according to edit distance. For random sequences with up
to $15\%$ uniform errors, the runtime of \astarpa scales near-linearly to very
long sequences ($10^7\bp$) and outperforms other exact aligners. We demonstrate
that on real ONT reads from a human genome, \astarpa is faster than other
aligners on only a limited portion of the reads.

subquadratically with length, and chaining to scale with error rate.
We
demontrate that the additional information from the whole sequence can improve
the scaling with query length, reference size and error rate, substantially
decrease the necessary computations, and result in algorithms that are orders of
magnitude faster than existing optimal algorithms.

As it turns out, when the error rate is limited, our optimal solutions
empirically scale near-linearly up to very long sequences. This translates to
many orders of magnitude of runtime speedup compared to state-of-the-art optimal
aligners.

The trie index allows scaling sublinearly with the
reference size, substantially increasing performance on large genomes.